from pathlib import Path

import requests
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Confirm, Prompt
from rich.table import Table
import time
import subprocess

def select_from_menu(
    console: Console,
    options: list[str],
    title: str,
    default_index: int = 0
) -> str | None:
    """
    Ags:
        options: ì„ íƒ ê°€ëŠ¥í•œ ì˜µì…˜ë“¤
        title: ë©”ë‰´ ì œëª©
        show_numbers: ë²ˆí˜¸ í‘œì‹œ ì—¬ë¶€
        default_index: ê¸°ë³¸ ì„ íƒ ì¸ë±ìŠ¤
    Returns:
        ì„ íƒëœ ì˜µì…˜ ì¸ë±ìŠ¤, ì·¨ì†Œì‹œ None
    """

    if not options:
        return None

    console.print(f"\n[bold cyan]{title}[/bold cyan]")

    table = Table(box=None, padding=(0, 2))
    table.add_column("ë²ˆí˜¸", style="cyan", width=4)
    table.add_column("ì˜µì…˜", style="white")

    for i, option in enumerate(options, 1):
        style = "bold green"
        table.add_row(f"[cyan]{i}[/cyan]", f"[{style}]{option}[/{style}]")

    console.print(table)

    # ì„ íƒì§€ ìƒì„±
    choices = [str(i) for i in range(1, len(options) + 1)]
    choices.append("q")

    choice = Prompt.ask(
        "ë²ˆí˜¸ë¥¼ ì„ míƒí•˜ì„¸ìš” (q: ì·¨ì†Œ)",
        choices=choices,
        default=str(default_index + 1)
    )

    if choice == "q":
        return None

    return options[int(choice) - 1]


class OllamaSetup:
    def __init__(self, console: Console):
        self.console = console
        self.settings_path = Path.cwd() / ".ocli" / "settings.json"
        self.api_url = "http://localhost:11434/api"

    def check_ollama_installation(self) -> bool:
        try:
            result = subprocess.run(
                ["ollama", "--version"],
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.returncode == 0
        except (subprocess.SubprocessError, FileNotFoundError):
            return False

    def check_ollama_service(self) -> bool:
        self.console.print("[bold green]ğŸ” Ollama ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤...[/bold green]")
        try:
            response = requests.get("http://localhost:11434/api/version", timeout=3)
            return response.status_code == 200
        except requests.RequestException:
            return False

    def get_available_ollama_models(self) -> list[str]:
        try:
            result = subprocess.run(
                ["ollama", "list"],
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode != 0:
                return []

            models = []
            lines = result.stdout.strip().split('\n')[1:]  # í—¤ë” ì œì™¸
            for line in lines:
                if line.strip():
                    model_name = line.split()[0]
                    models.append(model_name)

            return models
        except (subprocess.SubprocessError, FileNotFoundError):
            return []

    def show_install_recommend_ollama_model(self):
        recommended_models = [
            ('gpt-oss:20b', 'OpenAIâ€™s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. (14GB)', ),
            ('deepseek-r1:8b', 'DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro. (5.2GB)'),
            ('gemma3:4b', 'The current, most capable model that runs on a single GPU. (3.3GB)')
        ]

        self.console.print("\n[cyan]ğŸ“¦ ê¶Œì¥ Ollama ëª¨ë¸:[/cyan]")

        table = Table()
        table.add_column('ë²ˆí˜¸', style='cyan')
        table.add_column('ëª¨ë¸ëª…', style='green')
        table.add_column('ì„¤ëª…', style='white')

        for i, (model, description) in enumerate(recommended_models, 1):
            table.add_row(str(i), model, description)

        self.console.print(table)

        choice = Prompt.ask(
            "ì„¤ì¹˜í•  ëª¨ë¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1-3, ë˜ëŠ” 's'ë¡œ ê±´ë„ˆë›°ê¸°)",
            choices=["1", "2", "3", "s"],
            default="1"
        )

        if choice == 's':
            self.console.print("[yellow]ëª¨ë¸ ì„¤ì¹˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.[/yellow]")
            return False

        model_to_install = recommended_models[int(choice) - 1][0]

        self.console.print(f"\n[yellow]ğŸ“¥ {model_to_install} ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ì¤‘...[/yellow]")
        self.console.print("[dim]ì´ ì‘ì—…ì€ ëª‡ ë¶„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.[/dim]")

        try:
            # ollama pull ëª…ë ¹ì–´ ì‹¤í–‰
            process = subprocess.Popen(
                ["ollama", "pull", model_to_install],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            # ì‹¤ì‹œê°„ ì¶œë ¥
            if process.stdout is not None:
                for line in process.stdout:
                    self.console.print(f"  {line.strip()}")

            process.wait()

            if process.returncode == 0:
                self.console.print(f"\n[green]âœ“ {model_to_install} ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤![/green]")
                return True
            else:
                self.console.print(f"\n[red]âŒ ëª¨ë¸ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.[/red]")
                return False

        except subprocess.SubprocessError as e:
            self.console.print(f"\n[red]âŒ ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨: {e}[/red]")
            return False

    def show_ollama_installation_guide(self):
        self.console.print("[red]âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.[/red]")

        """Ollama ì„¤ì¹˜ ê°€ì´ë“œ í‘œì‹œ"""
        guide = """
[bold cyan]Ollama ì„¤ì¹˜ ë°©ë²•:[/bold cyan]

[yellow]macOS:[/yellow]
curl -fsSL https://ollama.com/install.sh | sh

[yellow]Linux:[/yellow]
curl -fsSL https://ollama.com/install.sh | sh

[yellow]Windows:[/yellow]
https://ollama.com/download/windows ì—ì„œ ë‹¤ìš´ë¡œë“œ

ì„¤ì¹˜ í›„ í„°ë¯¸ë„ì—ì„œ 'ollama serve'ë¥¼ ì‹¤í–‰í•˜ì—¬ ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•˜ì„¸ìš”.
        """

        panel = Panel(
            guide,
            title="ğŸ¦™ Ollama ì„¤ì¹˜ ê°€ì´ë“œ",
            border_style="green"
        )
        self.console.print(panel)

    def run_model(self, model: str):
        self.console.print(f"[bold green]ğŸ¤– {model} ëª¨ë¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤...[/bold green]")
        try:
            subprocess.run(["ollama", "run", model], check=True)
            self.console.print(f"[green]âœ“ {model} ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.[/green]")
        except subprocess.CalledProcessError:
            self.console.print(f"[red]âŒ {model} ëª¨ë¸ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.[/red]")

    def run_ollama_serve(self) -> bool:
        self.console.print("[yellow]ğŸš€ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...[/yellow]")

        subprocess.Popen(
            ['ollama', 'serve'],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True
        )

        for i in range(10):
            time.sleep(1)
            if self.check_ollama_service():
                return True

        return False

    def load_model(self, model_name: str) -> bool:
        self.console.print(f"[yellow]ğŸ”„ {model_name} ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...[/yellow]")

        try:
            response = requests.post(
                f"{self.api_url}/generate",
                json={
                    "model": model_name,
                    "prompt": "Hello",
                    "stream": False
                },
                timeout=30
            )

            if response.status_code == 200:
                self.console.print(f"[green]âœ“ {model_name} ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.[/green]")
                return True
            else:
                self.console.print(f"[red]âŒ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {response.status_code}[/red]")
                return False

        except Exception as e:
            self.console.print(f"[red]âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}[/red]")
            return False

    def setup_ai_providers(self):
        """AI Provider ì„¤ì • í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        self.console.print("[bold green]ğŸ¤– Ollama ê°€ ì„¤ì¹˜ë˜ì–´ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤...[/bold green]")

        # 1. Ollama í™•ì¸
        ollama_installed: bool = self.check_ollama_installation()
        if not ollama_installed:
            self.console.print("[red]âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.[/red]")
            self.show_ollama_installation_guide()
            return False

        self.console.print("[green]âœ“ Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.[/green]")
        self.console.print("[bold green]ğŸ” Ollama ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤...[/bold green]")
        ollama_running = self.check_ollama_service()

        if not ollama_running:
            self.console.print("[yellow]âš ï¸ Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/yellow]")
            return False

        self.console.print("[green]âœ“ Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.[/green]")

        self.console.print("[bold green]ğŸ“¦ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•©ë‹ˆë‹¤...[/bold green]")

        models: list[str] = self.get_available_ollama_models()
        selected_model = select_from_menu(self.console, models, "ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸", default_index=0)
        if selected_model is None:
            return False

        serve = self.run_ollama_serve()
        if not serve:
            self.console.print("[red]âŒ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.[/red]")
            return

        load_model = self.load_model(selected_model)
        if not load_model:
            self.console.print("[red]âŒ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.[/red]")
            return False

        return True